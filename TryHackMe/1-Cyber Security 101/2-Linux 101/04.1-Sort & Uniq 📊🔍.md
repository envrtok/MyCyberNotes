## `sort` Command ğŸ“ˆ

### Basic Usage
```bash
# Sort a file alphabetically (default) ğŸ”¤
sort filename.txt

# Sort and display output ğŸ‘€
sort file.txt

# Sort in reverse order ğŸ”„
sort -r file.txt

# Sort numerically ğŸ”¢
sort -n file.txt

# Sort by specific field (space/tab separated) ğŸ¯
sort -k 2 file.txt        # Sort by 2nd column
sort -k 3,3 file.txt      # Sort by 3rd column only
```

### Common Options âš™ï¸
```bash
# Remove duplicates while sorting ğŸ—‘ï¸
sort -u file.txt

# Case-insensitive sort ğŸ” 
sort -f file.txt

# Sort by numeric value of specific field ğŸ”¢ğŸ¯
sort -k 2n file.txt       # Sort 2nd column numerically

# Multiple field sorting ğŸ¯ğŸ¯
sort -k 1,1 -k 2n file.txt  # Sort by 1st field, then 2nd numerically

# Check if already sorted âœ…
sort -c file.txt          # Exit code 0 if sorted, 1 if not

# Human-readable numbers (K, M, G suffixes) ğŸ‘¨â€ğŸ’¼
sort -h file.txt

# Month name sorting ğŸ“…
sort -M file.txt
```

### Practical Examples ğŸ› ï¸
```bash
# Sort a CSV by 3rd column numerically ğŸ“Š
sort -t, -k3n data.csv

# Sort by multiple columns: name then age ğŸ‘¤ğŸ”¢
sort -k1,1 -k2,2n people.txt

# Sort IP addresses ğŸŒ
sort -t. -k1,1n -k2,2n -k3,3n -k4,4n ip_list.txt

# Sort with custom delimiter ğŸ”§
sort -t':' -k3n /etc/passwd
```

---

## `uniq` Command ğŸ¦„

### Basic Usage
```bash
# Remove consecutive duplicate lines ğŸ—‘ï¸
uniq file.txt

# Show only unique lines (appear exactly once) ğŸ’
uniq -u file.txt

# Show only duplicate lines ğŸ”„
uniq -d file.txt

# Count occurrences of each line ğŸ“Š
uniq -c file.txt
```

### Common Options âš™ï¸
```bash
# Case-insensitive comparison ğŸ” 
uniq -i file.txt

# Skip first N fields (space/tab separated) â­ï¸
uniq -f 2 file.txt       # Skip first 2 fields

# Skip first N characters â­ï¸
uniq -s 5 file.txt       # Skip first 5 characters

# Compare only first N characters âœ‚ï¸
uniq -w 10 file.txt      # Compare only first 10 chars
```

### Important Note âš ï¸
`uniq` only removes **consecutive** duplicates. Use with `sort` for complete deduplication:
```bash
sort file.txt | uniq
```

---

## Powerful Combinations ğŸ’ªğŸ”—

### Most Common Patterns
```bash
# Get unique sorted list ğŸ’ğŸ“ˆ
sort file.txt | uniq

# Count unique occurrences (sorted by count) ğŸ“ŠğŸ¥‡
sort file.txt | uniq -c | sort -nr

# Find duplicate lines in a file ğŸ”„ğŸ”
sort file.txt | uniq -d

# Find unique lines only ğŸ’âœ¨
sort file.txt | uniq -u
```

### Advanced Examples ğŸš€
```bash
# Top 10 most frequent lines ğŸ†ğŸ”Ÿ
sort file.txt | uniq -c | sort -nr | head -10

# Remove duplicates from unsorted data ğŸ—‘ï¸âœ¨
sort file.txt | uniq > deduplicated.txt

# Case-insensitive unique count ğŸ” ğŸ“Š
sort -f file.txt | uniq -ic

# Find duplicate IPs in log file ğŸŒğŸ”
cut -d' ' -f1 access.log | sort | uniq -d

# Count unique values in column 2 of CSV ğŸ“ŠğŸ¯
cut -d, -f2 data.csv | sort | uniq -c
```

### Real-world Use Cases ğŸŒ
```bash
# Analyze web server logs ğŸŒğŸ“Š
cut -d' ' -f1 access.log | sort | uniq -c | sort -nr

# Find duplicate files by checksum ğŸ“ğŸ”
find . -type f -exec md5sum {} \; | sort | uniq -d -w32

# Process CSV data ğŸ“ŠğŸ”§
cut -d, -f3 employees.csv | sort | uniq -c

# Clean word list ğŸ“âœ¨
sort words.txt | uniq > clean_words.txt

# Find most common error messages ğŸš¨ğŸ“Š
grep "ERROR" app.log | sort | uniq -c | sort -nr | head -5
```

## Quick Reference Table ğŸ“‹

| Command                       | Purpose                       | Common Use             |
| ----------------------------- | ----------------------------- | ---------------------- |
| `sort file`                   | Sort alphabetically           | Basic sorting          |
| `sort -n file`                | Sort numerically              | Number lists           |
| `sort -r file`                | Reverse sort                  | Z-A or high-low        |
| `sort -u file`                | Sort and remove duplicates    | Clean data             |
| `uniq file`                   | Remove consecutive duplicates | Process sorted data    |
| `uniq -c file`                | Count occurrences             | Frequency analysis     |
| `sort \| uniq`                | Remove all duplicates         | Complete deduplication |
| `sort \| uniq -c \| sort -nr` | Show frequency ranking        | Top N analysis         |

## Pro Tips ğŸ’¡

1. **Always remember**: `uniq` needs sorted input! ğŸ“ˆ â†’ ğŸ¦„
2. **Pipe magic**: Combine with `head`, `tail`, `grep` for powerful data analysis ğŸ”—âœ¨
3. **Memory efficient**: Great for large files that can't fit in memory ğŸ’¾ğŸš€
4. **Debug**: Use `sort -c` to check if your data is already sorted ğŸ›âœ…

## Memory Aid ğŸ§ 
> "**Sort** first, then **uniq** - that's the workflow trick!" ğŸµ
> 
> "**Consecutive** duplicates only - that's uniq's story!" ğŸ“–

Remember: **Always sort before uniq** unless you specifically want only consecutive duplicates removed! âš ï¸âœ¨

**Happy data processing!** ğŸ‰ğŸ§